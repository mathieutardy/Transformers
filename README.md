# Transformers

This is an exercise from a NLP course from CentraleSupelec. In this excercise, we loaded pretrained models (like the T5 model), wrote decoding algorithms, and interpreted the decisions made by transformer using attention visualisation. 

The cool thing about the T5 is that it was trained jointly on several supervised and unsupervised tasks such as Language Modelling, Machine Translation, Text Summarization and Question Answering. All tasks are "text" to "text" problems. 

In this exercise, we use three datasets to study the performance of three tasks: Machine Translation, Summarization and Question Answering. 

