# Transformers

This is an exercise from a Natural Language Processing course at CentraleSupelec. In this exercise, we loaded pre-trained models (like the T5 model), wrote decoding algorithms, and interpreted the decisions made by the transformer using attention visualisation.

T5 is interesting as it was trained jointly on several supervised and unsupervised tasks such as Language Modelling, Machine Translation, Text Summarization and Question Answering. All tasks are "text" to "text" problems.

Additionally, We use three datasets to study the performance of three tasks: Machine Translation, Summarisation and Question Answering.

