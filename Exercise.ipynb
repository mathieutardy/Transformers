{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "rhfU41hHEn8Z"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install sacrerouge sacrebleu bert-score\n",
    "\n",
    "!pip install sentencepiece\n",
    "!git clone https://github.com/huggingface/transformers.git\n",
    "!pip install ./transformers/.\n",
    "\n",
    "!pip install mosestokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "jH7L4BxrtFkV"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install --reinstall libxml-libxml-perl\n",
    "!sacrerouge setup-metric rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8sdvF17Bvjj"
   },
   "source": [
    "**Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDU2YhdmCESf"
   },
   "source": [
    "# **Deliverable 1.1 : T5 model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtSpRWIxCScr"
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to('cuda')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69Yfw4SpCUAT"
   },
   "source": [
    "## **Bible para dataset for translation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FN1fQu5PCaeQ"
   },
   "outputs": [],
   "source": [
    "prefix = \"translate English to French: \"\n",
    "lang1 = 'en'\n",
    "lang2 = 'fr'\n",
    "test_size = 0.1\n",
    "dataset_size = 100\n",
    "\n",
    "def create_batch(dataset, lang1=lang1, lang2=lang2, tokenizer=tokenizer, prefix=prefix, max_len=512):\n",
    "  batch_input = [prefix + i[lang1] for i in dataset]\n",
    "  batch_output = [i[lang2] for i in dataset]\n",
    "  input_encodings = tokenizer.batch_encode_plus(batch_input, padding=True, max_length=max_len, truncation=True, return_tensors=\"pt\").input_ids\n",
    "  target_encodings = tokenizer.batch_encode_plus(batch_output, padding=True, max_length=max_len, truncation=True, return_tensors=\"pt\").input_ids\n",
    "  return batch_input, batch_output, input_encodings.to('cuda'), target_encodings.to('cuda')\n",
    "\n",
    "bible_para = load_dataset('bible_para', lang1=lang1, lang2=lang2)['train']['translation']\n",
    "\n",
    "train_set, test_set = train_test_split(bible_para, test_size=0.1)\n",
    "test_set = test_set[:dataset_size]\n",
    "\n",
    "test_input_bible, test_target_bible, test_input_encodings_bible, test_output_encodings_bible = create_batch(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0EEnH4eCaUw",
    "outputId": "e6ede2c8-3ca5-476a-e6f7-c0b2fb63b741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "generated\tMaintenant, la terre était sans forme et vide, la fébrilité était sur la surface du profond, l'Esprit de Dieu a établi un arrière-plan sur la surface des eaux.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: God said, \"Let there be light,\" and there was light.\n",
      "target:\tDieu dit: Que la lumière soit! Et la lumière fut.\n",
      "generated\tDieu disait : « La lumière ne s'en trouve pas », et il y a de la lumière.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: God saw the light, and saw that it was good. God divided the light from the darkness.\n",
      "target:\tDieu vit que la lumière était bonne; et Dieu sépara la lumière d`avec les ténèbres.\n",
      "generated\tDieu a vu la lumière, et a vu qu'elle était bonne, Dieu a divisé la lumière de l'obscurité.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: God called the light \"day,\" and the darkness he called \"night.\" There was evening and there was morning, one day.\n",
      "target:\tDieu appela la lumière jour, et il appela les ténèbres nuit. Ainsi, il y eut un soir, et il y eut un matin: ce fut le premier jour.\n",
      "generated\tDieu a appelé la lumière «jour» et l'obscurité qu'il a appelée «no».\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test bible dataset\n",
    "result_bible = model.generate(test_input_encodings_bible, max_length=512)\n",
    "model_output_decoded_bible = tokenizer.batch_decode(result_bible, skip_special_tokens=True)\n",
    "i=0\n",
    "for input, prediction, true in zip(test_input_bible, model_output_decoded_bible, test_target_bible):\n",
    "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated\\t{}\\n------\\n\".format(input,true,prediction, model_prediction))\n",
    "  i+=1\n",
    "  if i == 5:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2Ubs76FCfsz"
   },
   "source": [
    "## **Xsum dataset for summarization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uCkqXR-CnNN"
   },
   "outputs": [],
   "source": [
    "prefix = \"summarize :\"\n",
    "dataset_size = 100\n",
    "\n",
    "def create_batch(dataset, dataset_size=dataset_size, prefix=prefix, tokenizer=tokenizer, max_len=512):\n",
    "    batch_input = [prefix + elt for elt in dataset['document'][:dataset_size]]\n",
    "    batch_output = [elt for elt in dataset['summary'][:dataset_size]]\n",
    "    input_encodings = tokenizer.batch_encode_plus(batch_input, padding=True, max_length=max_len, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    target_encodings = tokenizer.batch_encode_plus(batch_output, padding=True, max_length=max_len, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    return batch_input, batch_output, input_encodings.to('cuda'), target_encodings\n",
    "\n",
    "xsum = load_dataset('xsum')\n",
    "train_set = xsum['train']\n",
    "test_set = xsum['test']\n",
    "\n",
    "test_set = test_set[:dataset_size]\n",
    "\n",
    "test_input_cnn, test_target_cnn, test_input_encodings_cnn, test_output_encodings_cnn = create_batch(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZI2PTgeXCsv7",
    "outputId": "2224d816-6e40-40f9-b717-7505e04f0ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t summarize :Fast forward about 20 years, and it's fair to say he has done just that.\n",
      "The business he runs, Frasers Hospitality, is one of the world's biggest providers of high-end serviced apartments. Its 148 properties span about 80 capital cities, as well as financial hubs across Europe, Asia, the Middle East and Africa.\n",
      "But it almost didn't get off the ground.\n",
      "When Mr Choe was appointed to launch and lead the company, Asia was booming; the tiger economies of Hong Kong, South Korea, Taiwan and Singapore were expanding rapidly.\n",
      "But as Frasers prepared to open its first two properties in Singapore, the Asian financial crisis hit.\n",
      "It was 1997. Currencies went into freefall. Suddenly, people were losing their jobs and stopped travelling.\n",
      "Mr Choe recalls asking staff if they really wanted to continue working with the firm, because when the properties opened they might not get paid.\n",
      "\"It was really that serious,\" he says. \"I remember tearing up because they said 'let's open it, let's open it whether you can pay us or not'.\"\n",
      "Survival, Mr Choe admits, came through a bit of luck, and the misfortune of others.\n",
      "He had convinced the board at parent firm, property group Frasers Centrepoint, to open serviced apartments rather than hotels - partly because getting planning permission in Singapore was easier.\n",
      "But he also sensed it was a big, untapped market. And at the time of the crisis, it proved to be exactly what customers wanted.\n",
      "\"As we were going through this difficult patch, there were protests and riots in Jakarta,\" he says. \"A lot of companies like Microsoft called up looking for rooms for their staff because they were moving out of Jakarta.\"\n",
      "Frasers' 412 apartments were quickly in demand. Occupancy soon hit 70%, and then 90%.\n",
      "Explaining the popularity of serviced apartments, Mr Choe says that if people are staying somewhere for just a few days, they happily stay in hotels, but if they are going to be somewhere for one month to eight months, the walls of hotel rooms \"close in on you\".\n",
      "But now, Mr Choe, 57, faces new challenges - the travel tastes of millennials and the disruptive nature of Airbnb.\n",
      "\"The way to tackle Airbnb is not to ignore it. I will never underestimate Airbnb,\" he says.\n",
      "There's been no significant impact on Frasers yet. Big corporations still prefer to put employees in big service apartments, he says, because they can guarantee a level of safety and security. But that is likely to change, Mr Choe admits.\n",
      "\"I have two daughters who to my chagrin use Airbnb,\" he says. \"We took a family trip to Florence and I stayed in this wonderful boutique hotel, but paid a bundle for it.\n",
      "\"When my daughter joined us, she said, 'I'm just staying next door and paying about 80 euros'. We paid about 330 euros.\n",
      "\"I asked why they stayed at Airbnb. They say 'it's like a surprise, it's part of the adventure'.\"\n",
      "And so now, Mr Choe wants to bring some of that vibrancy to Frasers.\n",
      "While neutral colours, beige curtains and dark wooden chairs dominate its more traditional apartments, many customers want something different, and this is shaping Fraser's strategy.\n",
      "In 2015 it bought Malmaison Hotel du Vin, a UK hotel group that specialises in developing heritage properties into upscale boutique hotels.\n",
      "That has taken them beyond financial centres, including to Shakespeare's hometown of Stratford-upon-Avon. Or, an intrepid traveller with $500 (Â£325) to spend could have a night in a converted medieval prison in Oxford.\n",
      "And Frasers has launched the Capri sub-brand - whose website promises \"inspiring art and inspirational tech\".\n",
      "On a day-to-day basis Mr Choe says he still draws on his experience as a young man, who - having been given a scholarship by the Shangri-La hotel group to study at Cornell University in the US - came back to Asia to learn about the hospitality industry.\n",
      "\"They put me in every department conceivable. I remember one of the toughest jobs I had was in the butchery. I had to carve an entire cow. For one month, I could not eat meat.\n",
      "\"I'm thankful for those experiences. When you step into a hotel, you immediately pick up what works and what doesn't work.\n",
      "\"When I see the check-in staff walking more than three steps, I know the counter is set up wrong.\n",
      "\"It's like a cockpit. Can you imagine if the pilot had to turn around when he flies?\"\n",
      "More The Boss features, which every week profile a different business leader from around the world:\n",
      "The 'diva of divorce' for the world's super rich\n",
      "The snacks boss with an appetite for success\n",
      "Taking his own path: The world's leading maze designer\n",
      "Mr Choe adds that loyalty is very important to him, and he remains tremendously grateful to staff who have stayed with him.\n",
      "\"I will always respect and remember those who gave up their jobs to join me,\" he says.\n",
      "This loyalty is something that Mr Choe has earned, according to Donald MacLaurin, associate professor at Singapore Institute of Technology, and specialist in the hospitality sector.\n",
      "Mr MacLaurin points out that Mr Choe introduced a five-day working week, in a part of the world where six days is common, thereby showing \"a focus on quality of life issues for employees\".\n",
      "The associate professor adds says the early success of the business was remarkable given the timing of its launch.\n",
      "Fast forward to today and the company is now on track to operate 30,000 serviced apartments globally by 2019. That success, say Mr Choe's admirers, should make him something of a visionary.\n",
      "Follow The Boss series editor Will Smale on Twitter.\n",
      "target:\tOn the first day in his new job, Choe Peng Sum was given a fairly simple brief: \"Just go make us a lot of money.\"\n",
      "generated\tyes\n",
      "------\n",
      "\n",
      "source:\t summarize :\"The accident meant the motorway was closed, making travel to Mourneview Park impossible for the team and fans travelling from Belfast,\" said the Irish Football Association.\n",
      "A new date for the match has yet to be confirmed by Uefa.\n",
      "Northern Ireland have three points from their first two Group Six qualifiers.\n",
      "target:\tThe Women's Euro 2017 qualifier between Northern Ireland and the Czech Republic in Lurgan on Friday was postponed after a serious accident on the M1.\n",
      "generated\tno travel to the park was made possible for the team and fans. no match has been confirmed.\n",
      "------\n",
      "\n",
      "source:\t summarize :The Sunday Times says the missile veered off course during a test in June last year - weeks before the Commons voted to spend £40bn renewing Trident.\n",
      "Questioned by Andrew Marr, the PM refused to say four times if she had known about the test ahead of the vote.\n",
      "The SNP's Nicola Sturgeon called for a \"full disclosure\" of what happened.\n",
      "According to the Sunday Times, an unarmed Trident II D5 missile veered off in the wrong direction towards the US - instead of towards Africa - when it was launched from a British submarine off the coast of Florida.\n",
      "In July - days after Mrs May had become prime minister - MPs voted overwhelmingly in favour of replacing Trident.\n",
      "During the debate, Mrs May told MPs it would be \"an act of gross irresponsibility\" for the UK to abandon its nuclear weapons.\n",
      "MPs backed its renewal by 472 votes to 117. However, all 52 SNP MPs voted against it - as did Labour leader Jeremy Corbyn.\n",
      "When asked on the BBC's Andrew Marr Show whether she had known then that a misfire had happened, Mrs May said: \"I have absolute faith in our Trident missiles.\n",
      "\"When I made that speech in the House of Commons, what we were talking about was whether or not we should renew our Trident.\"\n",
      "She was asked a further three times - but did not answer the questions.\n",
      "The Ministry of Defence did not give details of the test process but said it had been a success.\n",
      "Scottish First Minister, Mrs Sturgeon - a long-standing opponent of Trident, whose submarines are based at Faslane, on the River Clyde - said the apparent misfire was a \"hugely serious issue\".\n",
      "She tweeted: \"There should be full disclosure of what happened, who knew what/when, and why the House of Commons wasn't told.\"\n",
      "Meanwhile, Mr Corbyn said the reports called for \"a serious discussion\".\n",
      "He told Sky News: \"It's a pretty catastrophic error when a missile goes in the wrong direction, and while it wasn't armed, goodness knows what the consequences of that could have been.\"\n",
      "Nia Griffith, the shadow defence secretary, said it was \"completely unacceptable\" that Mrs May had \"side-stepped\" questions.\n",
      "She called for the prime minister to give \"a full explanation\" to Parliament on Monday.\n",
      "Admiral Lord West, the Labour peer and ex-Royal Navy officer, said it was \"bizarre and stupid\" to not tell anyone about the test.\n",
      "The Campaign for Nuclear Disarmament (CND) described reports of a misfire as a \"very serious failure\".\n",
      "\"There's absolutely no doubt that this would have impacted on the debate in Parliament on Trident replacement,\" its general secretary Kate Hudson said.\n",
      "A statement issued by both Downing Street and the MoD said the capability and effectiveness of Trident was \"unquestionable\".\n",
      "\"In June the Royal Navy conducted a routine, unarmed Trident missile test launch from HMS Vengeance, as part of an operation which is designed to certify the submarine and its crew.\n",
      "\"Vengeance and her crew were successfully tested and certified, allowing Vengeance to return into service. We have absolute confidence in our independent nuclear deterrent.\"\n",
      "The Sunday Times says the test fire was launched from HMS Vengeance.\n",
      "It says the Trident II D5 missile was intended to be fired 5,600 miles (9,012 km) to a sea target off the west coast of Africa but veered towards the US.\n",
      "The cause remains top secret, the paper says, but it quotes a senior naval source as saying the missile suffered an in-flight malfunction after launching out of the water.\n",
      "HMS Vengeance, one of the UK's four Vanguard-class submarines, returned to sea for trials in December 2015 after a £350m refit, which included the installation of new missile launch equipment and upgraded computer systems.\n",
      "According to the Sunday Times, it is expected that Defence Secretary Michael Fallon will be called to the Commons to answer questions from MPs.\n",
      "BBC defence correspondent Jonathan Beale said while the MoD has described the test as a success for the crew and the boat, it has not denied the report that the missile itself might have veered off course.\n",
      "In the past the MoD has issued a press release and video of successful tests but its silence on this occasion has raised questions as to whether any fault was deliberately kept quiet ahead of the key vote, our correspondent added.\n",
      "The Trident system was acquired by the Thatcher government in the early 1980s as a replacement for the Polaris missile system, which the UK had possessed since the 1960s.\n",
      "Trident came into use in the 1990s. There are three parts to it - submarines, missiles and warheads. Although each component has years of use left, they cannot last indefinitely.\n",
      "The current generation of four submarines would begin to end their working lives some time in the late 2020s.\n",
      "A guide to the Trident debate\n",
      "target:\tTheresa May is coming under pressure to say whether she knew about a reported misfire of the UK's nuclear weapons system before a crucial Commons vote.\n",
      "generated\tyes\n",
      "------\n",
      "\n",
      "source:\t summarize :A spokesman for Palm Beach Gardens police in Florida confirmed to the BBC they were investigating a fatal crash involving the Grand Slam champion.\n",
      "A man was taken to hospital after the accident on 9 June and died two weeks later from his injuries, he said.\n",
      "According to TMZ, which broke the story, police believe the seven-time Grand Slam champion was at fault.\n",
      "But a lawyer for Williams said it was an \"unfortunate accident\".\n",
      "The man who died, Jerome Barson, was travelling with his wife who was driving their vehicle through an intersection when the accident happened.\n",
      "Williams' car suddenly darted into their path and was unable to clear the junction in time due to traffic jams, according to witness statements in a police report obtained by US media.\n",
      "Mrs Barson was also taken to hospital but survived.\n",
      "\"[Williams] is at fault for violating the right of way of [the other driver],\" the report said, adding that there were no other factors like drugs, alcohol or mobile phone distractions.\n",
      "The 37-year-old tennis star reportedly told police she did not see the couple's car and she was driving slowly.\n",
      "Police spokesman Major Paul Rogers said police were investigating whether the incident was connected to Mr Barson's death.\n",
      "Williams' lawyer Malcolm Cunningham told CNN in a statement: \"Ms Williams entered the intersection on a green light. The police report estimates that Ms Williams was travelling at 5mph when Mrs Barson crashed into her.\n",
      "\"Authorities did not issue Ms Williams with any citations or traffic violations. This is an unfortunate accident and Venus expresses her deepest condolences to the family who lost a loved one.\"\n",
      "Next week, Williams is due to play at Wimbledon in London, where she is seeded 10th.\n",
      "target:\tUS tennis star Venus Williams has been involved in a car accident that led to the death of a 78-year-old man.\n",
      "generated\tno\n",
      "------\n",
      "\n",
      "source:\t summarize :The International Tribunal for the Law of the Sea did, however, allow Ghana to continue developing current oilfields.\n",
      "These include the so called-Ten fields, part owned by UK firm Tullow Oil.\n",
      "Ivory Coast had asked that all drilling be suspended, but the tribunal ruled this would risk \"considerable financial loss\" to Ghana.\n",
      "Instead, it told the West African nation to \"take all necessary steps to ensure than no new drilling either by Ghana or under its control takes place in the disputed area\" and to \"refrain from granting any new permit for oil exploration and exploitation in the disputed area\".\n",
      "The area is believed to contain large reserves of oil, which both countries are keen to exploit.\n",
      "Pending a ruling on the precise maritime border, the tribunal told both countries to \"pursue co-operation and refrain from any unilateral action that might lead to aggravating the dispute\".\n",
      "The ruling was welcomed by Tullow Oil, which owns a large stake in the near $5bn (Â£3.3bn) Ten project, which could reportedly produce up to 80,000 barrels a day.\n",
      "\"Following this ruling, the Ten project can move ahead and we will now await instructions from the government of Ghana with regard to implementing those provisional measures that have been ordered by [the tribunal],\" a Tullow spokesman said.\n",
      "target:\tGhana has been told by an international tribunal not to begin any new offshore drilling for oil in disputed waters with the Ivory Coast.\n",
      "generated\tno new drilling by the country or under its control takes place in the disputed area. the international tribunal for the law of the sea has allowed the country to continue developing current oilfields. the ruling was welcomed by the firm, which owns a large stake in the $5bn (£3.3bn)\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test xsum dataset\n",
    "result_cnn = model.generate(test_input_encodings_cnn, max_length=512)\n",
    "model_output_decoded_cnn = tokenizer.batch_decode(result_cnn, skip_special_tokens=True)\n",
    "i=0\n",
    "for input, prediction, true in zip(test_input_cnn, model_output_decoded_cnn, test_target_cnn):\n",
    "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated\\t{}\\n------\\n\".format(input,true,prediction, model_prediction))\n",
    "  i+=1\n",
    "  if i == 5:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkC_CmiYCtb9"
   },
   "source": [
    "## **Test boolq dataset for question answering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khQBuJGaCxj_"
   },
   "outputs": [],
   "source": [
    "dataset_size = 100\n",
    "\n",
    "def create_batch(dataset, dataset_size=dataset_size, prefix=prefix, tokenizer=tokenizer, max_len=512):\n",
    "    batch_input = [\"question : \" + question + ' context : ' + passage \n",
    "                   for question, passage in zip(dataset['question'][:dataset_size], \n",
    "                                                dataset['passage'][:dataset_size])]\n",
    "    batch_output = [str(elt) for elt in dataset['answer'][:dataset_size]]\n",
    "    input_encodings = tokenizer.batch_encode_plus(batch_input, padding=True, max_length=max_len, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    target_encodings = tokenizer.batch_encode_plus(batch_output, padding=True, max_length=max_len, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    return batch_input, batch_output, input_encodings.to('cuda'), target_encodings.to('cuda')\n",
    "\n",
    "boolq = load_dataset('boolq')\n",
    "train_set = boolq['train']\n",
    "test_set = boolq['validation']\n",
    "\n",
    "test_set = test_set[:dataset_size]\n",
    "\n",
    "test_input_boolq, test_target_boolq, test_input_encodings_boolq, test_output_encodings_boolq = create_batch(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDizjLQLCx69",
    "outputId": "4fa0bb96-60a6-419d-88b3-09a65a711eae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t question : does ethanol take more energy make that produces context : All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\n",
      "target:\tFalse\n",
      "generated\tno\n",
      "------\n",
      "\n",
      "source:\t question : is house tax and property tax are same context : Property tax or 'house tax' is a local tax on buildings, along with appurtenant land. It is and imposed on the Possessor (not the custodian of property as per 1978, 44th amendment of constitution). It resembles the US-type wealth tax and differs from the excise-type UK rate. The tax power is vested in the states and is delegated to local bodies, specifying the valuation method, rate band, and collection procedures. The tax base is the annual rental value (ARV) or area-based rating. Owner-occupied and other properties not producing rent are assessed on cost and then converted into ARV by applying a percentage of cost, usually four percent. Vacant land is generally exempt. Central government properties are exempt. Instead a 'service charge' is permissible under executive order. Properties of foreign missions also enjoy tax exemption without requiring reciprocity. The tax is usually accompanied by service taxes, e.g., water tax, drainage tax, conservancy (sanitation) tax, lighting tax, all using the same tax base. The rate structure is flat on rural (panchayat) properties, but in the urban (municipal) areas it is mildly progressive with about 80% of assessments falling in the first two brackets.\n",
      "target:\tTrue\n",
      "generated\tyes\n",
      "------\n",
      "\n",
      "source:\t question : is pain experienced in a missing body part or paralyzed area context : Phantom pain sensations are described as perceptions that an individual experiences relating to a limb or an organ that is not physically part of the body. Limb loss is a result of either removal by amputation or congenital limb deficiency. However, phantom limb sensations can also occur following nerve avulsion or spinal cord injury.\n",
      "target:\tTrue\n",
      "generated\tyes\n",
      "------\n",
      "\n",
      "source:\t question : is harry potter and the escape from gringotts a roller coaster ride context : Harry Potter and the Escape from Gringotts is an indoor steel roller coaster at Universal Studios Florida, a theme park located within the Universal Orlando Resort. Similar to dark rides, the roller coaster utilizes special effects in a controlled-lighting environment and also employs motion-based 3-D projection of both animation and live-action sequences to enhance the experience. The ride, which is themed to the Gringotts Wizarding Bank, became the flagship attraction for the expanded Wizarding World of Harry Potter when it opened on July 8, 2014.\n",
      "target:\tTrue\n",
      "generated\tyes\n",
      "------\n",
      "\n",
      "source:\t question : is there a difference between hydroxyzine hcl and hydroxyzine pam context : Hydroxyzine preparations require a doctor's prescription. The drug is available in two formulations, the pamoate and the dihydrochloride or hydrochloride salts. Vistaril, Equipose, Masmoran, and Paxistil are preparations of the pamoate salt, while Atarax, Alamon, Aterax, Durrax, Tran-Q, Orgatrax, Quiess, and Tranquizine are of the hydrochloride salt.\n",
      "target:\tTrue\n",
      "generated\tno\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test boolq dataset\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-small-finetuned-boolq\").to('cuda')\n",
    "\n",
    "result_boolq = model.generate(test_input_encodings_boolq, max_length=512)\n",
    "model_output_decoded_boolq = tokenizer.batch_decode(result_boolq, skip_special_tokens=True)\n",
    "i=0\n",
    "for input, prediction, true in zip(test_input_boolq, model_output_decoded_boolq, test_target_boolq):\n",
    "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated\\t{}\\n------\\n\".format(input,true,prediction, model_prediction))\n",
    "  i+=1\n",
    "  if i == 5:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpctZoYQLTrr"
   },
   "source": [
    "# **Deliverable 1.2 : Rouge and Bleu implementation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGolj47xEjZ6"
   },
   "source": [
    "## **Rouge metric implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo_HHd-hNJXc"
   },
   "source": [
    "### **Implementation and test of rouge-N on Xsum dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HLFo3Ns1Ieml"
   },
   "outputs": [],
   "source": [
    "# Implement metric rouge\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from mosestokenizer import MosesTokenizer\n",
    "\n",
    "from sacrerouge.metrics import Rouge\n",
    "rouge_true = Rouge(max_ngram=3, compute_rouge_l=True)\n",
    "\n",
    "\n",
    "def rouge_n(true, pred, n):\n",
    "    vectorizer = CountVectorizer(lowercase=False, ngram_range=(n, n))\n",
    "    vectorizer.fit([true] + [pred])  \n",
    "\n",
    "    true_counter = vectorizer.transform([true]).toarray()[0]\n",
    "    pred_counter = vectorizer.transform([pred]).toarray()[0]\n",
    "\n",
    "    matches = 0\n",
    "    for true_elt, pred_elt in zip(true_counter, pred_counter):\n",
    "      matches += min(true_elt, pred_elt)\n",
    "\n",
    "    return matches/np.sum(true_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "FFCZxb2rchQ0"
   },
   "outputs": [],
   "source": [
    "# Test for rouge-n score\n",
    "\n",
    "rouge_n_dict = {}\n",
    "rouge_ns_dict = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "  rouge_n_list = []\n",
    "  rouge_ns_list = []\n",
    "  for true, pred in zip(test_target_cnn, model_output_decoded_cnn):\n",
    "    rouge_n_list.append(rouge_n(true, pred, i)*100)\n",
    "    rouge_ns_list.append(rouge_true.score(pred, [true])[f'rouge-{i}']['f1'])\n",
    "  rouge_n_dict[f'rouge-{i}'] = np.mean(rouge_n_list)\n",
    "  rouge_ns_dict[f'rouge-{i}'] = np.mean(rouge_ns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG0ofabkNSbQ"
   },
   "source": [
    "### **Implementation and test of rouge-L on Xsum dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "hKynDHBo-dWm"
   },
   "outputs": [],
   "source": [
    "# Own implementation of rouge-L\n",
    "def lcs(a, b):\n",
    "    \"\"\"\n",
    "    Compute the length of the longest common subsequence between two sequences.\n",
    "    \"\"\"\n",
    "    if len(a) < len(b):\n",
    "        a, b = b, a\n",
    "    if len(b) == 0:\n",
    "        return 0\n",
    "    row = [0] * len(b)\n",
    "    for ai in a:\n",
    "        left = 0\n",
    "        diag = 0\n",
    "        for j, bj in enumerate(b):\n",
    "            up = row[j]\n",
    "            if ai == bj:\n",
    "                value = diag + 1\n",
    "            else:\n",
    "                value = max(left, up)\n",
    "            row[j] = value\n",
    "            left = value\n",
    "            diag = up\n",
    "    return left\n",
    "\n",
    "def rouge_l(true, pred):\n",
    "    tokenizer = MosesTokenizer('en')\n",
    "    true_tokens = tokenizer(true)\n",
    "    pred_tokens = tokenizer(pred)\n",
    "    matches = lcs(true_tokens, pred_tokens)\n",
    "    return f1_score(matches, len(true_tokens), len(pred_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "FgQNOPDLUxFt"
   },
   "outputs": [],
   "source": [
    "# Test for rouge-l score (comparison between our implementation and sacrerouge \n",
    "# implementations)\n",
    "rouge_l_list = []\n",
    "rouge_ls_list = []\n",
    "\n",
    "for true, pred in zip(test_target_cnn, model_output_decoded_cnn):\n",
    "  rouge_l_list.append(rouge_l(true, pred)*100)\n",
    "  rouge_ls_list.append(rouge_true.score(pred, [true])['rouge-l']['f1'])\n",
    "\n",
    "rouge_n_dict['rouge-l'] = np.mean(rouge_l_list)\n",
    "rouge_ns_dict['rouge-l'] = np.mean(rouge_ls_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zk02YAjNUXE"
   },
   "source": [
    "### **Dataframe comparaison between our implementation and sacrerouge for ROUGE metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "qpQdj_-yl5qU",
    "outputId": "103b8e74-f38b-4c10-d8d8-271955411504"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-3</th>\n",
       "      <th>rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>own_implementation</th>\n",
       "      <td>3.07823</td>\n",
       "      <td>0.177866</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.638141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sacre-rouge</th>\n",
       "      <td>3.14784</td>\n",
       "      <td>0.245940</td>\n",
       "      <td>0.05128</td>\n",
       "      <td>2.156430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    rouge-1   rouge-2  rouge-3   rouge-l\n",
       "own_implementation  3.07823  0.177866  0.00000  2.638141\n",
       "sacre-rouge         3.14784  0.245940  0.05128  2.156430"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe comparaison for ROUGE\n",
    "import pandas as pd\n",
    "df_rouge = pd.DataFrame.from_records([rouge_n_dict, rouge_ns_dict], \n",
    "                                     index=['own_implementation', 'sacre-rouge'])\n",
    "df_rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFyZL2FoMh60"
   },
   "source": [
    "As, we can see our implementation of the Rouge metric is not exactly the same as the one coded in sacrerouge (for the rouge-n and rouge-l). However, it is quite close (0.1% difference), maybe it is because we don't use the same tokenizer to split the sentences in token.\n",
    "Moreover, concerning the xsum dataset, it doesn't give great results, as we could have expected when we looked on some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr2Cv1i2Er4Y"
   },
   "source": [
    "## **Bleu metric implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zWgATQleig8y"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def compute_pn(refs, candidate, n):\n",
    "    vectorizer = CountVectorizer(lowercase=False, ngram_range=(n, n))\n",
    "    vectorizer.fit(refs + candidate)  \n",
    "    a = vectorizer.transform(refs).toarray()\n",
    "    b = vectorizer.transform(candidate).toarray()\n",
    "\n",
    "    sum = np.sum(a*b)\n",
    "    sum_b = np.sum(b)\n",
    "    return sum / sum_b\n",
    "\n",
    "def compute_BP(refs, candidate):\n",
    "    r = np.sum([len(c.split()) for c in refs])\n",
    "    c = np.sum([len(c.split()) for c in candidate])\n",
    "    if c > r:\n",
    "      return 1\n",
    "    else:\n",
    "      return np.exp((1-r)/c)\n",
    "\n",
    "def BLEU_star(ref, candidate, N, w):\n",
    "    s = 0\n",
    "    for n in range(1, N+1):\n",
    "      pn = compute_pn([ref], [candidate], n)\n",
    "      s += w*np.log(pn)\n",
    "    BP = compute_BP([ref], [candidate])\n",
    "    return 100*BP * np.exp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJw3lexvfYTa",
    "outputId": "2f6233ee-05d5-4ffc-8809-0ef16f6d26bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own implementation :  13.53100109010626\n",
      "Sacrebleu implementation :  14.316443259052077\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# Test of our BLEU implementation metric on the bible dataset\n",
    "N = 4\n",
    "bleu_score = []\n",
    "for input, prediction, true in zip(test_input_bible, model_output_decoded_bible, test_target_bible):\n",
    "  bleu_score.append(BLEU_star(true, prediction, N, 1/N))\n",
    "\n",
    "# Test of the sacrebleu BLEU implementation on the bible dataset\n",
    "bleu = sacrebleu.corpus_bleu(model_output_decoded_bible, [test_target_bible]).score\n",
    "\n",
    "# Comparison\n",
    "print('Own implementation : ', np.mean(bleu_score))\n",
    "print('Sacrebleu implementation : ', bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_X0UM-U_RlMJ"
   },
   "source": [
    "As ROUGE, we have a difference between the results of our own implementation of BLEU and the implementation of sacrebleu, it can be because of the tokenizer.\n",
    "However, the results on the bible test set is not very high compared to our expectations seeing the first translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRVoIJjLFRcc"
   },
   "source": [
    "## **F1-score for question answenring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "cwosFDvaFV4S"
   },
   "outputs": [],
   "source": [
    "# This change in the answer is needed as it is clear that 'yes' corresponds to\n",
    "# true in our application on the Boolq dataset.\n",
    "\n",
    "true_to_yes = {\n",
    "    'True': 'yes',\n",
    "    'False': 'no'\n",
    "}\n",
    "\n",
    "def get_score_for_boolq_dataset(true, pred):\n",
    "  true = true_to_yes[true]\n",
    "  if true == pred:\n",
    "    return 1\n",
    "  return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hVgCnmeHcZ8",
    "outputId": "ce9e913d-e5f5-4a60-e504-9e76c6e06cce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# Test our implementation of \"f1-score\"\n",
    "f1_score_boolq = []\n",
    "for true, pred in zip(test_target_boolq, model_output_decoded_boolq):\n",
    "  f1_score_boolq.append(get_score_for_boolq_dataset(true, pred))\n",
    "\n",
    "print(np.mean(f1_score_boolq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMM9n88LTAry"
   },
   "source": [
    "The metric for Boolq dataset was not very developped because it's a dataset where you answers true or False, so the f1-score corresponds to the number of time you're right or not.\n",
    "In the boolq dataset, we can see that the score is pretty high. It corresponds to what we saw on the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Daxb22aWp3"
   },
   "source": [
    "# **Deliverable 1.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGrxSLiyE7fn"
   },
   "source": [
    "## **Implement all decoders functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Sly-IubGQCOD"
   },
   "outputs": [],
   "source": [
    "# Implement beam search\n",
    "def beam_search(input_ids, nb_beams, max_len=512):\n",
    "  with torch.no_grad():\n",
    "    decoder_input_ids = torch.zeros((1, 1)).int().to(device)\n",
    "\n",
    "    sequences = [(decoder_input_ids, 1)]\n",
    "\n",
    "    next_tok = 2\n",
    "\n",
    "    while next_tok != 1 and sequences[0][0][0].size()[0] < max_len:\n",
    "      new_sequences = []\n",
    "\n",
    "      for decoder_input_id, prob in sequences:\n",
    "        output = model(input_ids=input_ids, decoder_input_ids=decoder_input_id)\n",
    "        probas = torch.softmax(output.logits[0, -1, :], dim=-1)\n",
    "        next_tok_probas, next_tok_list = torch.topk(probas, nb_beams)\n",
    "\n",
    "        for i in range(next_tok_probas.size()[0]):\n",
    "          next_tok_prob = next_tok_probas[i].item()\n",
    "          next_token = next_tok_list[i].view(1, 1)\n",
    "          new_decoder_input_ids = torch.cat([decoder_input_id, next_token], dim=1).to(device)\n",
    "\n",
    "          new_sequences.append((new_decoder_input_ids, prob*next_tok_prob))\n",
    "            \n",
    "      sequences = sorted(new_sequences, key=lambda x: x[1], reverse=True)[:nb_beams]\n",
    "\n",
    "\n",
    "      next_tok = sequences[0][0][0][-1].view(1).item()\n",
    "\n",
    "    \n",
    "  return tokenizer.batch_decode(sequences[0][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "T2rsupGzNTLV"
   },
   "outputs": [],
   "source": [
    "# Implement nucleus sampling\n",
    "def nucleus_sampling(input_ids, top_p, max_len=512):\n",
    "  with torch.no_grad():\n",
    "      decoder_input_ids = torch.zeros((1, 1)).int().to(device)\n",
    "      next_tok = decoder_input_ids\n",
    "\n",
    "      while next_tok.view(1).item() != 1 and decoder_input_ids.size()[1] < max_len:\n",
    "        output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "        \n",
    "        probas = torch.softmax(output.logits[0, -1, :], dim=-1)\n",
    "        \n",
    "        prob, prob_indices = probas.sort(descending=True)\n",
    "        cum_probas = prob.cumsum(dim=0)\n",
    "\n",
    "        probas[prob_indices[cum_probas > top_p]] = 0\n",
    "\n",
    "        if probas.sum() > 0:\n",
    "          probas = probas/probas.sum()\n",
    "          next_tok = torch.multinomial(probas, 1).view(1, 1)\n",
    "       \n",
    "        else:\n",
    "          next_tok = prob_indices[0].view(1, 1)\n",
    "        \n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_tok], dim=1).to(device)\n",
    "  return tokenizer.batch_decode(decoder_input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WplSkZBKTndX"
   },
   "outputs": [],
   "source": [
    "# Implement temperature samp\n",
    "def temperature_sampling(input_ids, temperature, max_len=512):\n",
    "  with torch.no_grad():\n",
    "      decoder_input_ids = torch.zeros((1, 1)).int().to(device)\n",
    "      next_tok = decoder_input_ids\n",
    "\n",
    "      while next_tok.view(1).item() != 1 and decoder_input_ids.size()[1] < max_len:\n",
    "        output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "        \n",
    "        probas = torch.softmax(output.logits[0, -1, :]/temperature, dim=-1)\n",
    "\n",
    "        probas = probas/probas.sum()\n",
    "        next_tok = torch.multinomial(probas, 1)\n",
    "\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_tok.view(1,1)], \n",
    "                                      dim=1).to(device)\n",
    "\n",
    "  return tokenizer.batch_decode(decoder_input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-r0olaKlBrh"
   },
   "source": [
    "## **Tests translation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlZXRjfXlkN9"
   },
   "source": [
    "### **Test on text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGpQPWIMbBR0",
    "outputId": "4e9de2b2-ffc5-43fb-896d-87d6d6e258da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- n-beams : 1 ----------\n",
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "own generated\tAu début, Dieu créa les cieux et la terre.\n",
      "Model generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "own generated\tMaintenant, la terre était sans forme et vide, la fébrilité était sur la surface du profond, l'Esprit de Dieu a établi un aperçu de la surface des eaux.\n",
      "Model generated\tMaintenant, la terre était sans forme et vide, la fébrilité était sur la surface du profond, l'Esprit de Dieu a établi un aperçu de la surface des eaux.\n",
      "------\n",
      "\n",
      "------- n-beams : 2 ----------\n",
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "own generated\tAu début, Dieu créa les cieux et la terre.\n",
      "Model generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "own generated\tAujourd'hui, la terre était sans forme et vide, la foncée était sur la surface de la profondeur, l'Esprit de Dieu a plongé sur la surface des eaux.\n",
      "Model generated\tAujourd'hui, la terre était sans forme et vide, la foncée était sur la surface de la profondeur, l'Esprit de Dieu a plongé sur la surface des eaux.\n",
      "------\n",
      "\n",
      "------- n-beams : 3 ----------\n",
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "own generated\tAu début, Dieu créa les cieux et la terre.\n",
      "Model generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "own generated\tAujourd'hui, la terre était sans forme et vide, la foncée était sur la surface de la profondeur, l'Esprit de Dieu a plongé sur la surface des eaux.\n",
      "Model generated\tAujourd'hui, la terre était sans forme et vide, la foncée était sur la surface de la profondeur, l'Esprit de Dieu a plongé sur la surface des eaux.\n",
      "------\n",
      "\n",
      "------- n-beams : 4 ----------\n",
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "own generated\tAu début, Dieu créa les cieux et la terre.\n",
      "Model generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "own generated\tAujourd'hui, la terre était sans forme et vide, la foncée se trouvait sur la surface du profond, l'Esprit de Dieu s'élevait sur la surface des eaux.\n",
      "Model generated\tAujourd'hui, la terre était sans forme et vide, la foncée se trouvait sur la surface du profond, l'Esprit de Dieu s'établissait sur la surface des eaux.\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tests with n beams\n",
    "for n in range(1, 5):\n",
    "  i=0\n",
    "  print(f'------- n-beams : {n} ----------')\n",
    "  for input, true in zip(test_input_bible, test_target_bible):\n",
    "    input_ids = tokenizer.batch_encode_plus([input], padding=True, max_length=512, \n",
    "                                            truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prediction = beam_search(input_ids, n)[0]\n",
    "    model_prediction_ids = model.generate(input_ids, num_beams=n, max_length=512)\n",
    "    model_prediction = tokenizer.batch_decode(model_prediction_ids, skip_special_tokens=True)[0]\n",
    "    print(\"source:\\t {}\\ntarget:\\t{}\\nown generated\\t{}\\nModel generated\\t{}\\n------\\n\".format(input,true,prediction, model_prediction))\n",
    "    i+= 1\n",
    "    if i == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xavW8kqWire6",
    "outputId": "f0037db1-b0e4-4db2-f0da-5501a3886a43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- top-p : 0.5 ----------\n",
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "own generated\tAu début, Dieu créa les cieux et la terre.\n",
      "Model generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "own generated\tMaintenant, la terre était sans forme et vide, la foncée était sur la surface du profond, l'Esprit de Dieu s'élevait sur la surface des eaux.\n",
      "Model generated\tMaintenant, la terre était sans forme et vide, la fébrilité était sur la surface du profond, l'Esprit de Dieu a établi un aperçu de la surface des eaux.\n",
      "------\n",
      "\n",
      "------- top-p : 0.7 ----------\n",
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "own generated\tAu début, Dieu créa les cieux et la terre.\n",
      "Model generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "own generated\tEt le nom de la pétition qui est en lui apparaît ne constitue pas l'élément le plus important de la doctrine : il est le héros de la pensée, il est a sa mienne et il a édifié le pape d'esclavage en meurtrier.\n",
      "Model generated\tMaintenant, la terre était sans forme et vide, la fébrilité était sur la surface du profond, l'Esprit de Dieu a établi un aperçu de la surface des eaux.\n",
      "------\n",
      "\n",
      "------- top-p : 0.9 ----------\n",
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "own generated\tDurant le début, Dieu créa les cieux et la terre.\n",
      "Model generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "own generated\tDes conceptions ont surtout été faites au contenu du mot \"Woder\" parce que ces mots revêtent des considérations liées à la verve et à la gée des eaux.\n",
      "Model generated\tMaintenant, la terre était sans forme et vide, la fébrilité était sur la surface du profond, l'Esprit de Dieu a établi un aperçu de la surface des eaux.\n",
      "------\n",
      "\n",
      "------- top-p : 0.95 ----------\n",
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "own generated\tL’âge de 13 ans appartient. Depuis celle d’Isot, on ne le connaît pas, mais seulement parce qu’on veut pour peu en parler.\n",
      "Model generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "own generated\tMaintenant, il est le ciel sans forme et insolé, la dune pour la profondeur, l'esprit divin calme sur les profondeurs.\n",
      "Model generated\tMaintenant, la terre était sans forme et vide, la fébrilité était sur la surface du profond, l'Esprit de Dieu a établi un aperçu de la surface des eaux.\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tests with nuclear samping\n",
    "for top_p in [0.5, 0.7, 0.9, 0.95]:\n",
    "  i=0\n",
    "  print(f'------- top-p : {top_p} ----------')\n",
    "  for input, true in zip(test_input_bible, test_target_bible):\n",
    "    input_ids = tokenizer.batch_encode_plus([input], padding=True, max_length=512, \n",
    "                                            truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prediction = nucleus_sampling(input_ids, top_p)[0]\n",
    "    model_prediction_ids = model.generate(input_ids, top_p=top_p, max_length=512)\n",
    "    model_prediction = tokenizer.batch_decode(model_prediction_ids, skip_special_tokens=True)[0]\n",
    "    print(\"source:\\t {}\\ntarget:\\t{}\\nown generated\\t{}\\nModel generated\\t{}\\n------\\n\".format(input,true,prediction, model_prediction))\n",
    "    i+= 1\n",
    "    if i == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5BzdMc5kPrt",
    "outputId": "d52169c4-1445-4124-eae5-f5a68dc29210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- temperature : 0.5 ----------\n",
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "own generated\tAu début, Dieu a créé les cieux et la terre.\n",
      "Model generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "own generated\tAujourd'hui, la terre était sans forme et vide, la féroce s'est installée sur la surface de la profondeur, et l'Esprit de Dieu a plongé sur la surface des eaux.\n",
      "Model generated\tMaintenant, la terre était sans forme et vide, la fébrilité était sur la surface du profond, l'Esprit de Dieu a établi un aperçu de la surface des eaux.\n",
      "------\n",
      "\n",
      "------- temperature : 0.7 ----------\n",
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "own generated\tAu début Dieu créait les ciels et la terre.\n",
      "Model generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "own generated\tAujourd'hui, la terre était sans forme et vide, la faune se trouvait sur la surface profonde, l'Esprit de Dieu allait sur la surface des eaux.\n",
      "Model generated\tMaintenant, la terre était sans forme et vide, la fébrilité était sur la surface du profond, l'Esprit de Dieu a établi un aperçu de la surface des eaux.\n",
      "------\n",
      "\n",
      "------- temperature : 0.9 ----------\n",
      "source:\t translate English to French: In the beginning God created the heavens and the earth.\n",
      "target:\tAu commencement, Dieu créa les cieux et la terre.\n",
      "own generated\tPour le début, Dieu a créé les cieux et la terre.\n",
      "Model generated\tAu début, Dieu créa les cieux et la terre.\n",
      "------\n",
      "\n",
      "source:\t translate English to French: Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\n",
      "target:\tLa terre était informe et vide: il y avait des ténèbres à la surface de l`abîme, et l`esprit de Dieu se mouvait au-dessus des eaux.\n",
      "own generated\tCet espace ne se concentre pas sur le statut de personne qui connaît la Vie et conquiert une nouvelle profondeur et une innovation à la fois, que son existence subit la mortalité, l'autopédagogie à la cause d'un élu et que, les sinistres et les chef(e), ce fut souvent le temps de s'occuper de cet élu.\n",
      "Model generated\tMaintenant, la terre était sans forme et vide, la fébrilité était sur la surface du profond, l'Esprit de Dieu a établi un aperçu de la surface des eaux.\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tests with temperature with softmax\n",
    "for t in [0.5, 0.7, 0.9]:\n",
    "  i=0\n",
    "  print(f'------- temperature : {t} ----------')\n",
    "  for input, true in zip(test_input_bible, test_target_bible):\n",
    "    input_ids = tokenizer.batch_encode_plus([input], padding=True, max_length=512, \n",
    "                                            truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prediction = temperature_sampling(input_ids, t)[0]\n",
    "    model_prediction_ids = model.generate(input_ids, temperature=t, max_length=512)\n",
    "    model_prediction = tokenizer.batch_decode(model_prediction_ids, skip_special_tokens=True)[0]\n",
    "    print(\"source:\\t {}\\ntarget:\\t{}\\nown generated\\t{}\\nModel generated\\t{}\\n------\\n\".format(input,true,prediction, model_prediction))\n",
    "    i+= 1\n",
    "    if i == 2: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4eATwn-lndP"
   },
   "source": [
    "### **Test with BLEU metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GxM67iuHmZ9k"
   },
   "outputs": [],
   "source": [
    "dict_bleu_metric = {\n",
    "    'bleu_n_beams_1': [],\n",
    "    'bleu_n_beams_2' : [],\n",
    "    'bleu_n_beams_3' : [],\n",
    "    'bleu_n_beams_4' : [],\n",
    "    'bleu_top_p_0.5' : [],\n",
    "    'bleu_top_p_0.7' : [],\n",
    "    'bleu_top_p_0.9' : [],\n",
    "    'bleu_top_p_0.95' : [],\n",
    "    'bleu_top_p_0.5' : [],\n",
    "    'bleu_temp_0.5' : [],\n",
    "    'bleu_temp_0.7' : [],\n",
    "    'bleu_temp_0.9' : []\n",
    "    }\n",
    "\n",
    "for input, true in zip(test_input_bible, test_target_bible):\n",
    "  input_ids = tokenizer.batch_encode_plus([input], padding=True, max_length=512, \n",
    "                                          truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "  for n in [1, 2, 3, 4]:\n",
    "    prediction = beam_search(input_ids, n)[0]\n",
    "    dict_bleu_metric[f'bleu_n_beams_{n}'].append(BLEU_star(true, prediction, 4, 1/4))\n",
    "  \n",
    "  for top_p in [0.5, 0.7, 0.9, 0.95]:\n",
    "    prediction = nucleus_sampling(input_ids, top_p)[0]\n",
    "    dict_bleu_metric[f'bleu_top_p_{top_p}'].append(BLEU_star(true, prediction, 4, 1/4))\n",
    "  \n",
    "  for t in [0.5, 0.7, 0.9]:\n",
    "    prediction = temperature_sampling(input_ids, t)[0]\n",
    "    dict_bleu_metric[f'bleu_temp_{t}'].append(BLEU_star(true, prediction, 4, 1/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "mcMPa34zpOno",
    "outputId": "a14be8f9-0c6f-44c4-813f-6c90cfe48ef7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df_bleu = pd.DataFrame(dict_bleu_metric).reset_index()\n",
    "decoding_melted = pd.melt(df_bleu, id_vars=\"index\")\n",
    "plt.figure(figsize=(20,6))\n",
    "sns.barplot(data=decoding_melted, x=\"variable\", y=\"value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQD6udUqlXcp"
   },
   "source": [
    "## **Test summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZxvFNaslyga"
   },
   "source": [
    "### **Test with text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9L8wh9PpaPZ",
    "outputId": "3805d102-f860-4e74-8b31-3cb09d32a80c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- n-beams : 1 ----------\n",
      "source:\t summarize :Fast forward about 20 years, and it's fair to say he has done just that.\n",
      "The business he runs, Frasers Hospitality, is one of the world's biggest providers of high-end serviced apartments. Its 148 properties span about 80 capital cities, as well as financial hubs across Europe, Asia, the Middle East and Africa.\n",
      "But it almost didn't get off the ground.\n",
      "When Mr Choe was appointed to launch and lead the company, Asia was booming; the tiger economies of Hong Kong, South Korea, Taiwan and Singapore were expanding rapidly.\n",
      "But as Frasers prepared to open its first two properties in Singapore, the Asian financial crisis hit.\n",
      "It was 1997. Currencies went into freefall. Suddenly, people were losing their jobs and stopped travelling.\n",
      "Mr Choe recalls asking staff if they really wanted to continue working with the firm, because when the properties opened they might not get paid.\n",
      "\"It was really that serious,\" he says. \"I remember tearing up because they said 'let's open it, let's open it\n",
      "target:\tOn the first day in his new job, Choe Peng Sum was given a fairly simple brief: \"Just go make us a lot of money.\"\n",
      "own generated\tFrasers Hospitality is one of the world's biggest providers of serviced apartments. it is one of the world's biggest providers of high-end serviced apartments. the company is a global provider of services to millennials.\n",
      "Model generated\tFrasers Hospitality is one of the world's biggest providers of serviced apartments. it is one of the world's biggest providers of high-end serviced apartments. the company is a global provider of services to millennials.\n",
      "------\n",
      "\n",
      "source:\t summarize :\"The accident meant the motorway was closed, making travel to Mourneview Park impossible for the team and fans travelling from Belfast,\" said the Irish Football Association.\n",
      "A new date for the match has yet to be confirmed by Uefa.\n",
      "Northern Ireland have three points from their first two Group Six qualifiers.\n",
      "target:\tThe Women's Euro 2017 qualifier between Northern Ireland and the Czech Republic in Lurgan on Friday was postponed after a serious accident on the M1.\n",
      "own generated\tthe accident meant the motorway was closed. the match was a 'comfort' for the team and fans travelling from Belfast.\n",
      "Model generated\tthe accident meant the motorway was closed. the match was a 'comfort' for the team and fans travelling from Belfast.\n",
      "------\n",
      "\n",
      "source:\t summarize :The Sunday Times says the missile veered off course during a test in June last year - weeks before the Commons voted to spend £40bn renewing Trident.\n",
      "Questioned by Andrew Marr, the PM refused to say four times if she had known about the test ahead of the vote.\n",
      "The SNP's Nicola Sturgeon called for a \"full disclosure\" of what happened.\n",
      "According to the Sunday Times, an unarmed Trident II D5 missile veered off in the wrong direction towards the US - instead of towards Africa - when it was launched from a British submarine off the coast of Florida.\n",
      "In July - days after Mrs May had become prime minister - MPs voted overwhelmingly in favour of replacing Trident.\n",
      "During the debate, Mrs May told MPs it would be \"an act of gross irresponsibility\" for the UK to abandon its nuclear weapons.\n",
      "MPs backed its renewal by 472 votes to 117. However, all 52 SNP MPs voted against it - as did Labour leader Jeremy Corbyn.\n",
      "When asked on the BBC's Andrew Marr Show whether she had known then that a \n",
      "target:\tTheresa May is coming under pressure to say whether she knew about a reported misfire of the UK's nuclear weapons system before a crucial Commons vote.\n",
      "own generated\tthe Sunday Times says the missile veered off course during a test in June last year. the missile veered off course towards the us - instead of towards africa. the snp's Nicola Sturgeon called for a \"full disclosure\" of what happened.\n",
      "Model generated\tthe Sunday Times says the missile veered off course during a test in June last year. the missile veered off course towards the us - instead of towards africa. the snp's Nicola Sturgeon called for a \"full disclosure\" of what happened.\n",
      "------\n",
      "\n",
      "------- n-beams : 2 ----------\n",
      "source:\t summarize :Fast forward about 20 years, and it's fair to say he has done just that.\n",
      "The business he runs, Frasers Hospitality, is one of the world's biggest providers of high-end serviced apartments. Its 148 properties span about 80 capital cities, as well as financial hubs across Europe, Asia, the Middle East and Africa.\n",
      "But it almost didn't get off the ground.\n",
      "When Mr Choe was appointed to launch and lead the company, Asia was booming; the tiger economies of Hong Kong, South Korea, Taiwan and Singapore were expanding rapidly.\n",
      "But as Frasers prepared to open its first two properties in Singapore, the Asian financial crisis hit.\n",
      "It was 1997. Currencies went into freefall. Suddenly, people were losing their jobs and stopped travelling.\n",
      "Mr Choe recalls asking staff if they really wanted to continue working with the firm, because when the properties opened they might not get paid.\n",
      "\"It was really that serious,\" he says. \"I remember tearing up because they said 'let's open it, let's open it\n",
      "target:\tOn the first day in his new job, Choe Peng Sum was given a fairly simple brief: \"Just go make us a lot of money.\"\n",
      "own generated\tFrasers Hospitality is one of the world's biggest providers of high-end serviced apartments. it is one of the world's biggest providers of high-end serviced apartments. the company is one of the world's biggest providers of high-end serviced apartments.\n",
      "Model generated\tFrasers Hospitality is one of the world's biggest providers of high-end serviced apartments. it is one of the world's biggest providers of high-end serviced apartments. the company is one of the world's biggest providers of high-end serviced apartments.\n",
      "------\n",
      "\n",
      "source:\t summarize :\"The accident meant the motorway was closed, making travel to Mourneview Park impossible for the team and fans travelling from Belfast,\" said the Irish Football Association.\n",
      "A new date for the match has yet to be confirmed by Uefa.\n",
      "Northern Ireland have three points from their first two Group Six qualifiers.\n",
      "target:\tThe Women's Euro 2017 qualifier between Northern Ireland and the Czech Republic in Lurgan on Friday was postponed after a serious accident on the M1.\n",
      "own generated\tthe accident meant the motorway was closed, making travel to Mourneview Park impossible for the team and fans travelling from Belfast. a new date for the match has yet to be confirmed by Uefa.\n",
      "Model generated\tthe accident meant the motorway was closed, making travel to Mourneview Park impossible for the team and fans travelling from Belfast. a new date for the match has yet to be confirmed by Uefa.\n",
      "------\n",
      "\n",
      "source:\t summarize :The Sunday Times says the missile veered off course during a test in June last year - weeks before the Commons voted to spend £40bn renewing Trident.\n",
      "Questioned by Andrew Marr, the PM refused to say four times if she had known about the test ahead of the vote.\n",
      "The SNP's Nicola Sturgeon called for a \"full disclosure\" of what happened.\n",
      "According to the Sunday Times, an unarmed Trident II D5 missile veered off in the wrong direction towards the US - instead of towards Africa - when it was launched from a British submarine off the coast of Florida.\n",
      "In July - days after Mrs May had become prime minister - MPs voted overwhelmingly in favour of replacing Trident.\n",
      "During the debate, Mrs May told MPs it would be \"an act of gross irresponsibility\" for the UK to abandon its nuclear weapons.\n",
      "MPs backed its renewal by 472 votes to 117. However, all 52 SNP MPs voted against it - as did Labour leader Jeremy Corbyn.\n",
      "When asked on the BBC's Andrew Marr Show whether she had known then that a \n",
      "target:\tTheresa May is coming under pressure to say whether she knew about a reported misfire of the UK's nuclear weapons system before a crucial Commons vote.\n",
      "own generated\tthe Sunday Times says the missile veered off course during a test in June last year. the missile was launched from a British submarine off the coast of Florida. the missile veered off course towards the us - instead of towards africa.\n",
      "Model generated\tthe Sunday Times says the missile veered off course during a test in June last year. the missile was launched from a British submarine off the coast of Florida. the missile veered off course towards the us - instead of towards africa.\n",
      "------\n",
      "\n",
      "------- n-beams : 4 ----------\n",
      "source:\t summarize :Fast forward about 20 years, and it's fair to say he has done just that.\n",
      "The business he runs, Frasers Hospitality, is one of the world's biggest providers of high-end serviced apartments. Its 148 properties span about 80 capital cities, as well as financial hubs across Europe, Asia, the Middle East and Africa.\n",
      "But it almost didn't get off the ground.\n",
      "When Mr Choe was appointed to launch and lead the company, Asia was booming; the tiger economies of Hong Kong, South Korea, Taiwan and Singapore were expanding rapidly.\n",
      "But as Frasers prepared to open its first two properties in Singapore, the Asian financial crisis hit.\n",
      "It was 1997. Currencies went into freefall. Suddenly, people were losing their jobs and stopped travelling.\n",
      "Mr Choe recalls asking staff if they really wanted to continue working with the firm, because when the properties opened they might not get paid.\n",
      "\"It was really that serious,\" he says. \"I remember tearing up because they said 'let's open it, let's open it\n",
      "target:\tOn the first day in his new job, Choe Peng Sum was given a fairly simple brief: \"Just go make us a lot of money.\"\n",
      "own generated\tFrasers Hospitality is one of the world's biggest providers of high-end serviced apartments. its 148 properties span about 80 capital cities, as well as financial hubs across Europe, Asia, the Middle East and Africa. when the firm opened its first two properties in Singapore, the Asian financial crisis hit.\n",
      "Model generated\tFrasers Hospitality is one of the world's biggest providers of high-end serviced apartments. its 148 properties span about 80 capital cities, as well as financial hubs across Europe, Asia, the Middle East and Africa. when the firm opened its first two properties in Singapore, the Asian financial crisis hit.\n",
      "------\n",
      "\n",
      "source:\t summarize :\"The accident meant the motorway was closed, making travel to Mourneview Park impossible for the team and fans travelling from Belfast,\" said the Irish Football Association.\n",
      "A new date for the match has yet to be confirmed by Uefa.\n",
      "Northern Ireland have three points from their first two Group Six qualifiers.\n",
      "target:\tThe Women's Euro 2017 qualifier between Northern Ireland and the Czech Republic in Lurgan on Friday was postponed after a serious accident on the M1.\n",
      "own generated\tthe accident meant the motorway was closed, making travel to Mourneview Park impossible for the team and fans travelling from Belfast. a new date for the match has yet to be confirmed by Uefa.\n",
      "Model generated\tthe accident meant the motorway was closed, making travel to Mourneview Park impossible for the team and fans travelling from Belfast. a new date for the match has yet to be confirmed by Uefa.\n",
      "------\n",
      "\n",
      "source:\t summarize :The Sunday Times says the missile veered off course during a test in June last year - weeks before the Commons voted to spend £40bn renewing Trident.\n",
      "Questioned by Andrew Marr, the PM refused to say four times if she had known about the test ahead of the vote.\n",
      "The SNP's Nicola Sturgeon called for a \"full disclosure\" of what happened.\n",
      "According to the Sunday Times, an unarmed Trident II D5 missile veered off in the wrong direction towards the US - instead of towards Africa - when it was launched from a British submarine off the coast of Florida.\n",
      "In July - days after Mrs May had become prime minister - MPs voted overwhelmingly in favour of replacing Trident.\n",
      "During the debate, Mrs May told MPs it would be \"an act of gross irresponsibility\" for the UK to abandon its nuclear weapons.\n",
      "MPs backed its renewal by 472 votes to 117. However, all 52 SNP MPs voted against it - as did Labour leader Jeremy Corbyn.\n",
      "When asked on the BBC's Andrew Marr Show whether she had known then that a \n",
      "target:\tTheresa May is coming under pressure to say whether she knew about a reported misfire of the UK's nuclear weapons system before a crucial Commons vote.\n",
      "own generated\tthe Sunday Times says the missile veered off course during a test in June last year. the missile was launched from a British submarine off the coast of Florida. in July, MPs voted overwhelmingly in favour of replacing Trident.\n",
      "Model generated\tthe Sunday Times says the missile veered off course during a test in June last year. the missile was launched from a British submarine off the coast of Florida. in July, MPs voted overwhelmingly in favour of replacing Trident.\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tests with n beams\n",
    "for n in [1, 2, 4]:\n",
    "  i=0\n",
    "  print(f'------- n-beams : {n} ----------')\n",
    "  for input, true in zip(test_input_cnn, test_target_cnn):\n",
    "    input_ids = tokenizer.batch_encode_plus([input], padding=True, max_length=512, \n",
    "                                            truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prediction = beam_search(input_ids, n)[0]\n",
    "    model_prediction_ids = model.generate(input_ids, num_beams=n, max_length=512)\n",
    "    model_prediction = tokenizer.batch_decode(model_prediction_ids, skip_special_tokens=True)[0]\n",
    "    print(\"source:\\t {}\\ntarget:\\t{}\\nown generated\\t{}\\nModel generated\\t{}\\n------\\n\".format(input[:1000],true,prediction, model_prediction))\n",
    "    i+= 1\n",
    "    if i == 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_AIB05QlupN"
   },
   "source": [
    "### **Test with ROUGE metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "WSdtzw2kpjWl"
   },
   "outputs": [],
   "source": [
    "dict_rouge_metric = {\n",
    "    'rouge_n_beams_1': [],\n",
    "    'rouge_n_beams_2' : [],\n",
    "    'rouge_n_beams_4' : [],\n",
    "    'rouge_top_p_0.5' : [],\n",
    "    'rouge_top_p_0.7' : [],\n",
    "    'rouge_top_p_0.9' : [],\n",
    "    'rouge_top_p_0.95' : [],\n",
    "    'rouge_top_p_0.5' : [],\n",
    "    'rouge_temp_0.5' : [],\n",
    "    'rouge_temp_0.7' : [],\n",
    "    'rouge_temp_0.9' : []\n",
    "    }\n",
    "\n",
    "for input, true in zip(test_input_cnn, test_target_cnn):\n",
    "  input_ids = tokenizer.batch_encode_plus([input], padding=True, max_length=512, \n",
    "                                          truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "  for n in [1, 2, 4]:\n",
    "    prediction = beam_search(input_ids, n)[0]\n",
    "    score = rouge_true.score(prediction, [true])['rouge-l']['f1']\n",
    "    dict_rouge_metric[f'rouge_n_beams_{n}'].append(score)\n",
    "  \n",
    "  for top_p in [0.5, 0.7, 0.9, 0.95]:\n",
    "    prediction = nucleus_sampling(input_ids, top_p)[0]\n",
    "    score = rouge_true.score(prediction, [true])['rouge-l']['f1']\n",
    "    dict_rouge_metric[f'rouge_top_p_{top_p}'].append(score)\n",
    "  \n",
    "  for t in [0.5, 0.7, 0.9]:\n",
    "    prediction = temperature_sampling(input_ids, t)[0]\n",
    "    score = rouge_true.score(prediction, [true])['rouge-l']['f1']\n",
    "    dict_rouge_metric[f'rouge_temp_{t}'].append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "CWVkyADZ28Ol",
    "outputId": "1aa78b87-cb23-408c-930a-e3de6a576724"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df_bleu = pd.DataFrame(dict_rouge_metric).reset_index()\n",
    "decoding_melted = pd.melt(df_bleu, id_vars=\"index\")\n",
    "plt.figure(figsize=(20,6))\n",
    "sns.barplot(data=decoding_melted, x=\"variable\", y=\"value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aOWrlnlBGDL"
   },
   "source": [
    "### Test for question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1S3QQT22_nG",
    "outputId": "2ba05261-152e-4ee4-b1b3-9a7ef27ca366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- n-beams : 1 ----------\n",
      "source:\t question : does ethanol take more energy make that produces context : All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\n",
      "target:\tFalse\n",
      "own generated\tproduction of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended\n",
      "Model generated\tproduction of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended\n",
      "------\n",
      "\n",
      "source:\t question : is house tax and property tax are same context : Property tax or 'house tax' is a local tax on buildings, along with appurtenant land. It is and imposed on the Possessor (not the custodian of property as per 1978, 44th amendment of constitution). It resembles the US-type wealth tax and differs from the excise-type UK rate. The tax power is vested in the states and is delegated to local bodies, specifying the valuation method, rate band, and collection procedures. The tax base is the annual rental value (ARV) or area-based rating. Owner-occupied and other properties not producing rent are assessed on cost and then converted into ARV by applying a percentage of cost, usually four percent. Vacant land is generally exempt. Central government properties are exempt. Instead a 'service charge' is permissible under executive order. Properties of foreign missions also enjoy tax exemption without requiring reciprocity. The tax is usually accompanied by service taxes, e.g., water tax, drainage tax, conservancy (sanitation) tax, lighting tax, all using the same tax base. The rate structure is flat on rural (panchayat) properties, but in the urban (municipal) areas it is mildly progressive with about 80% of assessments falling in the first two brackets.\n",
      "target:\tTrue\n",
      "own generated\tproperty tax or 'house tax' is a local tax on buildings, along with appurtenant land\n",
      "Model generated\tproperty tax or 'house tax' is a local tax on buildings, along with appurtenant land\n",
      "------\n",
      "\n",
      "source:\t question : is pain experienced in a missing body part or paralyzed area context : Phantom pain sensations are described as perceptions that an individual experiences relating to a limb or an organ that is not physically part of the body. Limb loss is a result of either removal by amputation or congenital limb deficiency. However, phantom limb sensations can also occur following nerve avulsion or spinal cord injury.\n",
      "target:\tTrue\n",
      "own generated\tparalyzed area\n",
      "Model generated\tparalyzed area\n",
      "------\n",
      "\n",
      "------- n-beams : 2 ----------\n",
      "source:\t question : does ethanol take more energy make that produces context : All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\n",
      "target:\tFalse\n",
      "own generated\tproduction of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended\n",
      "Model generated\tproduction of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended\n",
      "------\n",
      "\n",
      "source:\t question : is house tax and property tax are same context : Property tax or 'house tax' is a local tax on buildings, along with appurtenant land. It is and imposed on the Possessor (not the custodian of property as per 1978, 44th amendment of constitution). It resembles the US-type wealth tax and differs from the excise-type UK rate. The tax power is vested in the states and is delegated to local bodies, specifying the valuation method, rate band, and collection procedures. The tax base is the annual rental value (ARV) or area-based rating. Owner-occupied and other properties not producing rent are assessed on cost and then converted into ARV by applying a percentage of cost, usually four percent. Vacant land is generally exempt. Central government properties are exempt. Instead a 'service charge' is permissible under executive order. Properties of foreign missions also enjoy tax exemption without requiring reciprocity. The tax is usually accompanied by service taxes, e.g., water tax, drainage tax, conservancy (sanitation) tax, lighting tax, all using the same tax base. The rate structure is flat on rural (panchayat) properties, but in the urban (municipal) areas it is mildly progressive with about 80% of assessments falling in the first two brackets.\n",
      "target:\tTrue\n",
      "own generated\tproperty tax or 'house tax' is a local tax on buildings, along with appurtenant land\n",
      "Model generated\tproperty tax or 'house tax' is a local tax on buildings, along with appurtenant land\n",
      "------\n",
      "\n",
      "source:\t question : is pain experienced in a missing body part or paralyzed area context : Phantom pain sensations are described as perceptions that an individual experiences relating to a limb or an organ that is not physically part of the body. Limb loss is a result of either removal by amputation or congenital limb deficiency. However, phantom limb sensations can also occur following nerve avulsion or spinal cord injury.\n",
      "target:\tTrue\n",
      "own generated\tparalyzed area\n",
      "Model generated\tparalyzed area\n",
      "------\n",
      "\n",
      "------- n-beams : 4 ----------\n",
      "source:\t question : does ethanol take more energy make that produces context : All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\n",
      "target:\tFalse\n",
      "own generated\tproduction of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended\n",
      "Model generated\tproduction of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended\n",
      "------\n",
      "\n",
      "source:\t question : is house tax and property tax are same context : Property tax or 'house tax' is a local tax on buildings, along with appurtenant land. It is and imposed on the Possessor (not the custodian of property as per 1978, 44th amendment of constitution). It resembles the US-type wealth tax and differs from the excise-type UK rate. The tax power is vested in the states and is delegated to local bodies, specifying the valuation method, rate band, and collection procedures. The tax base is the annual rental value (ARV) or area-based rating. Owner-occupied and other properties not producing rent are assessed on cost and then converted into ARV by applying a percentage of cost, usually four percent. Vacant land is generally exempt. Central government properties are exempt. Instead a 'service charge' is permissible under executive order. Properties of foreign missions also enjoy tax exemption without requiring reciprocity. The tax is usually accompanied by service taxes, e.g., water tax, drainage tax, conservancy (sanitation) tax, lighting tax, all using the same tax base. The rate structure is flat on rural (panchayat) properties, but in the urban (municipal) areas it is mildly progressive with about 80% of assessments falling in the first two brackets.\n",
      "target:\tTrue\n",
      "own generated\tproperty tax or 'house tax' is a local tax on buildings, along with appurtenant land\n",
      "Model generated\tproperty tax or 'house tax' is a local tax on buildings, along with appurtenant land\n",
      "------\n",
      "\n",
      "source:\t question : is pain experienced in a missing body part or paralyzed area context : Phantom pain sensations are described as perceptions that an individual experiences relating to a limb or an organ that is not physically part of the body. Limb loss is a result of either removal by amputation or congenital limb deficiency. However, phantom limb sensations can also occur following nerve avulsion or spinal cord injury.\n",
      "target:\tTrue\n",
      "own generated\tparalyzed area\n",
      "Model generated\tparalyzed area\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tests with n beams\n",
    "for n in [1, 2, 4]:\n",
    "  i=0\n",
    "  print(f'------- n-beams : {n} ----------')\n",
    "  for input, true in zip(test_input_boolq, test_target_boolq):\n",
    "    input_ids = tokenizer.batch_encode_plus([input], padding=True, max_length=512, \n",
    "                                            truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prediction = beam_search(input_ids, n)[0]\n",
    "    model_prediction_ids = model.generate(input_ids, num_beams=n, max_length=512)\n",
    "    model_prediction = tokenizer.batch_decode(model_prediction_ids, skip_special_tokens=True)[0]\n",
    "    print(\"source:\\t {}\\ntarget:\\t{}\\nown generated\\t{}\\nModel generated\\t{}\\n------\\n\".format(input,true,prediction, model_prediction))\n",
    "    i+= 1\n",
    "    if i == 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzNXXIkNlgRi"
   },
   "source": [
    "## **Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwDtN0pRrOi-"
   },
   "source": [
    "In this section, we implemented three different decoders for Natural Language generation : Translation, Summarization and Question answering. The three decoders are : n-beams decoder, nucleus sampling and temperature with Softmax sampling, parametrized respectively by : n-beams (number of beams), top-p (probability maximum of the cumulative sum of the probabilities) and temperature. \n",
    "We tested these decoders with different parametrization. Firstly, on a text-based aspect to see if the results were visually good (especially for translation and summarization). And then, we calculated metrics in order to evaluate these decoders. For the translation, we choose the BLEU metric that we created earlier on bible para dataset and for summarization, we choose the ROUGE-L metric implemented in scarerouge library on xsum dataset.\n",
    "\n",
    "For machine translation, the best decoder turned out to be : the n-beams decoder in particular the 2-beams and 4-beams. It was expected because the beams decoder works really fine with translation as it computes the best probabilities of a suite of words.\n",
    "\n",
    "For summarization, the best decoder turned out to be : the nucleus samping (in particular 0.5 for the top_p probability) and the n-beams look very great too. It can be due that summary is not about giving correct answers, so it has not a 'true' answer for this.\n",
    "\n",
    "For question answering, no decoders seems to work well on the boolq dataset.\n",
    "\n",
    "However, when we did some experiments with the three decoders, beam search reveals to be the most time consuming because beam search has an exponential complexity and therefore it becomes very time-consuming when n_beams increases. But, in the results, it turns out that when you increase the number of beans, it leads to better predictions. So, you have to find a compromise between rapidity and precision. Compared to the other 2 decoders that look pretty fast (as the sampling is done by pytorch and it is really optimized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NQO0b81T2a_"
   },
   "source": [
    "# Deliverable 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjJVE7tTVylk"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import numpy as np\n",
    "\n",
    "# functions for 2.1\n",
    "\n",
    "def draw(data, x, y, ax):\n",
    "    seaborn.heatmap(data, \n",
    "                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, \n",
    "                    cbar=False, ax=ax)\n",
    "    \n",
    "def get_attentions(model, tokenizer, sentence, sample_index = 0, limit=None):\n",
    "  # get ids of input\n",
    "  input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "  if limit:\n",
    "    input_ids = input_ids[0][:limit].reshape((1, limit))\n",
    "  # get ids of output\n",
    "  output_ids = tokenizer(output_decoded, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "  outputs = model(input_ids=input_ids, decoder_input_ids=output_ids, output_attentions=True)\n",
    "\n",
    "  ## Step 3: Convert input and output into tokens\n",
    "  input_id_list = input_ids[sample_index].tolist()\n",
    "  tokens_in = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "\n",
    "  output_id_list = output_ids[sample_index].tolist()\n",
    "  tokens_out = tokenizer.convert_ids_to_tokens(output_id_list)\n",
    "  return outputs, tokens_in, tokens_out\n",
    "\n",
    "def plot_cross_attention(outputs, tokens_in, tokens_out, aggregation=False):\n",
    "  if aggregation == True:\n",
    "    for layer in range(0, 6, 1):\n",
    "        fig, axs = plt.subplots(1,1, figsize=(10, 10))\n",
    "        print(\"\\nAttention Layer\", layer+1)\n",
    "        attention_sum = 0\n",
    "        for h in range(8):\n",
    "          attention_sum += outputs.cross_attentions[layer][0, h].cpu().data\n",
    "        draw(attention_sum, \n",
    "            tokens_in, tokens_out, ax=axs)\n",
    "        plt.show()\n",
    "  else:\n",
    "    for layer in range(0, 6, 1):\n",
    "        fig, axs = plt.subplots(1,8, figsize=(30, 20))\n",
    "        print(\"Attention Layer\", layer+1)\n",
    "        for h in range(8):\n",
    "            draw(outputs.cross_attentions[layer][0, h].cpu().data, \n",
    "                tokens_in, tokens_out if h == 0 else [], ax=axs[h])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrbRo4ZDVkQY"
   },
   "source": [
    "## Bible para dataset for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPTUs2vqoyBy"
   },
   "outputs": [],
   "source": [
    "# # Poor performer\n",
    "# sample_index = 46\n",
    "\n",
    "#Good performer\n",
    "sample_index = 56\n",
    "\n",
    "## Step 1: apply model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small',output_attentions=True).to('cuda')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "sentence = test_input_bible[sample_index]\n",
    "sentence_tokenized = test_input_encodings_bible[sample_index]\n",
    "# the translated sentence encoded (vector)\n",
    "output = model.generate(sentence_tokenized.reshape((1,len(sentence_tokenized))),\n",
    "                        max_length=512)\n",
    "# the translated sentence decoded (string)\n",
    "output_decoded = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "## Step 2 & 3: Recover info for vis\n",
    "outputs, tokens_in, tokens_out = get_attentions(model, tokenizer, sentence, sample_index = 0)\n",
    "\n",
    "## Step 4: Plot attention layers\n",
    "plot_cross_attention(outputs, tokens_in, tokens_out, aggregation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6jrFiX2spWL"
   },
   "outputs": [],
   "source": [
    "plot_cross_attention(outputs, tokens_in, tokens_out, aggregation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0_qniSjT35D"
   },
   "source": [
    "## CNN daily mail dataset for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jittHvoGOyCL"
   },
   "outputs": [],
   "source": [
    "# get a short text for visualisation purposes\n",
    "for idx, i in enumerate(test_input_cnn):\n",
    "  if len(i) < 500:\n",
    "    print(i)\n",
    "    print(idx)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EA3m8kOZO1MF"
   },
   "outputs": [],
   "source": [
    "## Step 1: apply model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small',output_attentions=True).to('cuda')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# We use this example for simplicity\n",
    "sentence = test_input_cnn[1]\n",
    "# sentence = 'summarize :Fast forward about 20 years, and its fair to say he has done just that.' \\\n",
    "#            'The business he runs, Frasers Hospitality, is one of the world biggest providers of high-end' \\\n",
    "#            'serviced apartments. Its 148 properties span about 80 capital cities, as well as financial hubs across Europe,' \\\n",
    "#            'Asia, the Middle East and Africa.\\nBut it almost didnt get off the ground'\n",
    "sentence_tokenized = test_input_encodings_cnn[sample_index]\n",
    "\n",
    "#---------------------\n",
    "# We have to limit the number of words in the input text for visualisation purposes\n",
    "limit = 50\n",
    "sentence_tokenized_small = sentence_tokenized #[:limit]\n",
    "#--------------------\n",
    "\n",
    "# the translated sentence encoded (vector)\n",
    "output = model.generate(sentence_tokenized_small.reshape((1,len(sentence_tokenized_small))),\n",
    "                        max_length=512)\n",
    "# the translated sentence decoded (string)\n",
    "output_decoded = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "## Step 2 & 3: Recover info for vis\n",
    "outputs, tokens_in, tokens_out = get_attentions(model, tokenizer, sentence,\n",
    "                                                sample_index = 0)\n",
    "\n",
    "## Step 4: Plot attention layers\n",
    "plot_cross_attention(outputs, tokens_in, tokens_out, aggregation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w6NdmT5ViAi"
   },
   "source": [
    "## Boolq data set for question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0XkWmgrVryS"
   },
   "outputs": [],
   "source": [
    "# get a short question for visualisation purposes\n",
    "for idx, i in enumerate(test_input_boolq):\n",
    "  if len(i) < 200:\n",
    "    print(i)\n",
    "    print(idx)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NU3ihJ3oO_cz"
   },
   "outputs": [],
   "source": [
    "sample_index = 8\n",
    "\n",
    "## Step 1: apply model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-small-finetuned-boolq\").to('cuda')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "sentence = test_input_boolq[sample_index]\n",
    "sentence_tokenized = test_input_encodings_boolq[sample_index]\n",
    "\n",
    "# We have to limit the number of words in the input text for visualisation purposes\n",
    "sentence_tokenized_small = sentence_tokenized[:30]\n",
    "\n",
    "# the translated sentence encoded (vector)\n",
    "output = model.generate(sentence_tokenized.reshape((1,len(sentence_tokenized))),\n",
    "                        max_length=512)\n",
    "# the translated sentence decoded (string)\n",
    "output_decoded = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "## Step 2 & 3: Recover info for vis\n",
    "outputs, tokens_in, tokens_out = get_attentions(model, tokenizer, sentence, sample_index = 0)\n",
    "\n",
    "## Step 4: Plot attention layers\n",
    "plot_cross_attention(outputs, tokens_in, tokens_out, aggregation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ9wGoYBPBCW"
   },
   "source": [
    "## Report\n",
    "\n",
    "For each tasks, we visualized the cross-attention matrices for each head and each layer or did an aggregation of all attention heads for each layer using a summation operation. For each task, aggregation is usually very useful to derive an intuition from the model attention scores as they are large variances between heads.\n",
    "\n",
    "The first element to consider is that for all tasks, the prefix of each input (in our case it is the task translate/answer/summarise) receives the most attention, which makes a lot of sense as the model will respond very differently depending on the task it performs.\n",
    "\n",
    "Then, each task has its specificities. For the translation problem, one may notice that when we aggregate attention scores, we notice a clear diagonal line in the cross-attention matrix at each layer when the model performs well (sample 56 has a BLEU score of 51/100) and no diagonal line when it does poorly (sample 46 has a BLEU score of 0). Moreover, it is interesting to note that tokens representing the end of a sentence (/s or .) get a lot of attention. This does make sense as the model will try to deduce synctactic and semantic relations to make a translation and knowing how to seperate sentences is crucial for this task.\n",
    "\n",
    "For summarisations, the cross-attention matrix is very sparse which is very sensible. It gives importance to words who are themselves selected as being good representation of the abstract itself.\n",
    "\n",
    "Finally, question answering are very similar to summarisation problem in the sense that words that are  useful in answering the question get highlighted. However, in most cases only one word is used as a predictor. This leads us to believe that the model would probably perform more poorly if information must be retrieved from several locations in the text.\n",
    "\n",
    "We could further aggregate cross-attention scores of each layers for even better to visualise these patterns even more clearly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-69WUb2PH8B"
   },
   "source": [
    "# Deliverable 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42IjVQa-YQId"
   },
   "source": [
    "From the paper: We define the “confidence” of a head as the average of its maximum attention weight excluding the end of sentence symbol, where averageis taken over tokens in a set of sentences used for evaluation (development set). A confident head is one that usually assigns a high proportion of its attention to a single token. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "executionInfo": {
     "elapsed": 927,
     "status": "ok",
     "timestamp": 1617551965063,
     "user": {
      "displayName": "Mathieu TARDY",
      "photoUrl": "",
      "userId": "14129872026631470354"
     },
     "user_tz": -480
    },
    "id": "sjF0_wJEl96-"
   },
   "outputs": [],
   "source": [
    "def plotting_confidence_score(datasetname,title):\n",
    "\n",
    "  #------------------------------ same base as 2.1 to get outputs\n",
    "  ## Step 1: apply model\n",
    "  model = T5ForConditionalGeneration.from_pretrained('t5-small',output_attentions=True).to('cuda')\n",
    "  tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "  #------------------------------\n",
    "  attention_matrices = []\n",
    "\n",
    "  for sample_index in range(len(datasetname)):\n",
    "    sentence = datasetname[sample_index]\n",
    "    sentence_tokenized = test_input_encodings_bible[sample_index]\n",
    "    # the translated sentence encoded (vector)\n",
    "    output = model.generate(sentence_tokenized.reshape((1,len(sentence_tokenized))),\n",
    "                            max_length=512)\n",
    "    ## Step 2 & 3: Recover info for vis\n",
    "    outputs, tokens_in, tokens_out = get_attentions(model, tokenizer, sentence, sample_index = 0)\n",
    "\n",
    "    array = []\n",
    "    for layer in range(len(outputs.cross_attentions)):\n",
    "      layer_max_scores = []\n",
    "      for heads in range(outputs.cross_attentions[layer].shape[1]):\n",
    "        layer_max_scores.append(torch.max(outputs.cross_attentions[layer][0,heads].cpu().data).numpy())\n",
    "      array.append(layer_max_scores)\n",
    "    attention_matrices.append(array)\n",
    "\n",
    "  attention = np.mean( np.array(attention_matrices), axis=0 )\n",
    "\n",
    "  # Plot heat map\n",
    "  heatmap = seaborn.heatmap(attention)\n",
    "  plt.ylabel(\"Layers\")\n",
    "  plt.xlabel(\"Heads\") \n",
    "  plt.title(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 74776,
     "status": "ok",
     "timestamp": 1617552041418,
     "user": {
      "displayName": "Mathieu TARDY",
      "photoUrl": "",
      "userId": "14129872026631470354"
     },
     "user_tz": -480
    },
    "id": "tqbMMZUya7q-",
    "outputId": "9d71bd8e-595a-4555-c304-14be8ce0e13a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Translation - Bible para dataset\n",
    "plotting_confidence_score(test_input_bible,\"Translation Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 81381,
     "status": "ok",
     "timestamp": 1617552122808,
     "user": {
      "displayName": "Mathieu TARDY",
      "photoUrl": "",
      "userId": "14129872026631470354"
     },
     "user_tz": -480
    },
    "id": "JOovUf3Ia9Qw",
    "outputId": "366306b7-c085-4290-f8ab-43ecb13b6685"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1310 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarisation - CNN daily mail dataset\n",
    "plotting_confidence_score(test_input_cnn,\"Summarisation Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 155902,
     "status": "ok",
     "timestamp": 1617552197338,
     "user": {
      "displayName": "Mathieu TARDY",
      "photoUrl": "",
      "userId": "14129872026631470354"
     },
     "user_tz": -480
    },
    "id": "irSGZOXsh14X",
    "outputId": "518d75dc-89e4-4f94-f0f2-4d8f261220bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (825 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question Answering - Boolq\n",
    "plotting_confidence_score(test_input_boolq,\"Q&A Attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFHy2fL4iv6a"
   },
   "source": [
    "## Report\n",
    "\n",
    "On the translation task, we can see that in most heads, the model outputs very high attention scores for at least one word. This is sensible as word like \"the\" and \"a\" are often straightforward to translate and thus have high attention scores. It seems that for this specific task most of heads help in determining important words, which are words that are likely to be the right match in the translation.\n",
    "\n",
    "For the summarisation task, only one head has on average high max attention score. It makes sense has it is very hard to say that a word is a good representation of an entire text. Summarisation will most often be a grouping of words which constitutes the summary and will rarely have a single word being a perfect candidate that summarises the text.\n",
    "\n",
    "For the question and answering task, the confidence score heatmap seems in the middle of the two above tasks. The average high max attention is fairly high for all heads. Again, this meets our expectations. The model usually have a good idea about which word consitutes the answer and therefore outputs high attention score. This validates our previous takeaway. Indeed, Q&A tasks tend to extract at least one important word that constitutes the answer."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nGolj47xEjZ6",
    "jo_HHd-hNJXc",
    "BG0ofabkNSbQ",
    "Wr2Cv1i2Er4Y",
    "SRVoIJjLFRcc",
    "bGrxSLiyE7fn"
   ],
   "name": "TP3_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
